# create venv, and pip install Scrapy
# to create new project

scrapy startproject <project-name>

# to create new spider

scrapy genspider <spider-name> <url-to-scrape>

# use scrapy shell to find the selectors you want

- pip install ipython
- add in 'scrapy.cfg' file under settings:
    shell = ipython
- run 'scrapy shell' in console
- once you have run the command, you can navigate through the website using your spider manually
- use 'fetch('website-url')' to get a response from a website
- you can figure out what you want through this and then code the spider

# to run a spider 'scrapy crawl <spider-name>' in the project directory in the console, not in the scrapy shell

# if you would like to output data in a csv, you can use other extensions or file types as well
scrapy crawl <spider-name> -O <file-name>.csv

To append data into a file instead of overwriting you can use '-o' instead of '-O'

Items helps us define what we want in a block of data we are scraping

Pipelines are used to process data before storing them in the output

# You can automate the output file format and output format as well, for which you will have to add something like this in the 'settings.py' file

FEEDS = {
    'booksdata.json': {
        'format': 'json',
    }
}

# You can specify custom FEED settings within the spider as well as follows:

# Within for example bookspider.py within the class:

custom_settings = {
        'FEEDS': {
            'booksdata2.csv': {'format': 'json'}
        }
    }

# You can set overwrite to True or False as well as follows:

custom_settings = {
        'FEEDS': {
            'booksdata2.csv': {'format': 'json',
            'overwrite': True
            }
        }
    }

# The above can be done within custom_settings and as well as the global settings.py

# Installing Python 3 and MySQL development headers and libraries:

sudo apt-get install python3-dev default-libmysqlclient-dev build-essential pkg-config

# You can refer to DB connection and adding items in the DB in the pipelines.py file

# You will have to add the pipeline which is adding data to DB into the settings.py as well

# You can use the following command to view Table columns in mysql

SELECT COLUMN_NAME
FROM INFORMATION_SCHEMA.columns
WHERE TABLE_NAME='<table-name>';

# While testing you can drop table to start anew as follows:

DROP TABLE <table-name>;

# What if scraping is blocked -> We need user agent

useragentstring.com

# Copy the user agent by inspecting from devtools into the website above to learn about all the details of a user agent for that site

# For some sites if we change the user agent each time we make a request website will think that it's a different user each time we make a request and so it will allow, however for more complex websites they will look at everything in the request header and all of it will need to be (slightly) different

# So how can we change user agents, ip addresses, and request headers?

# Disabling mysql pipeline for learning bypassing

# You can add user agents in the spider file as follows:

user_agent_list = [*populate list*]

# This can then be used as shown in the following example, add the user agent change line in every line which is making a response.follow:

yield response.follow(book_page_url, callback=self.parse_book_page, headers="User-Agent": user_agent_list[random.randint(0, len(user_agent_list))])

# You need A LOT of user agents, for this we can use the fake user agent api

# So to do that we can implement a middleware

# Scroll to the bottom of the middleware.py file to refer to how it is done using scrapeops.io

# Once you are done implementing the scrapeops middleware, add it in 'settings.py' in DOWNLOADER_MIDDLEWARE

# You no longer need to send the fake headers from the spider, since the middleware has been implemented now

# Sometimes for complex sites you have to set the ROBOTSTXT_OBEY to False in the settings.py